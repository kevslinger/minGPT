{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train GPT on gym\n",
    "\n",
    "Train a GPT model on a dedicated addition dataset to see if a Transformer can learn to add."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up logging\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make deterministic\n",
    "from mingpt.utils import set_seed\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('DiscreteGridworld-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4, 10], dtype=uint8)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import sys\n",
    "sys.path.append('../DRQN_pt')\n",
    "import gym, envs\n",
    "\n",
    "\n",
    "class GymDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Returns addition problems of up to some number of digits in the inputs. Recall\n",
    "    that all GPT cares about are sequences of integers, and completing them according to\n",
    "    patterns in the data. Therefore, we have to somehow encode addition problems\n",
    "    as a sequence of integers.\n",
    "    \n",
    "    The sum of two n-digit numbers gives a third up to (n+1)-digit number. So our\n",
    "    encoding will simply be the n-digit first number, n-digit second number, \n",
    "    and (n+1)-digit result, all simply concatenated together. Because each addition\n",
    "    problem is so structured, there is no need to bother the model with encoding\n",
    "    +, =, or other tokens. Each possible sequence has the same length, and simply\n",
    "    contains the raw digits of the addition problem.\n",
    "    \n",
    "    As a few examples, the 2-digit problems:\n",
    "    - 85 + 50 = 135 becomes the sequence [8, 5, 5, 0, 1, 3, 5]\n",
    "    - 6 + 39 = 45 becomes the sequence [0, 6, 3, 9, 0, 4, 5]\n",
    "    etc.\n",
    "    \n",
    "    We will also only train GPT on the final (n+1)-digits because the first\n",
    "    two n-digits are always assumed to be given. So when we give GPT an exam later,\n",
    "    we will e.g. feed it the sequence [0, 6, 3, 9], which encodes that we'd like\n",
    "    to add 6 + 39, and hope that the model completes the integer sequence with [0, 4, 5]\n",
    "    in 3 sequential steps.\n",
    "    \n",
    "    fun exercise: does it help if the result is asked to be produced in reverse order?\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, split: str, env_name: str =\"DiscreteGridworld-v0\"):\n",
    "        self.env = gym.make(env_name)\n",
    "        self.split = split # train/test\n",
    "        self.vocab_size = 12 # 12 possible... -1 through 10\n",
    "        # TODO: should be env.action_space.shape[0] as well instead of 1\n",
    "        self.block_size = self.env.observation_space.shape[0] * 5\n",
    "        \n",
    "        # split up all addition problems into either training data or test data\n",
    "        # Let's start with 50k samples\n",
    "        # num = (10**self.ndigit)**2 # total number of possible combinations\n",
    "        # r = np.random.RandomState(1337) # make deterministic\n",
    "        # perm = r.permutation(num)\n",
    "        # num_test = min(int(num*0.2), 1000) # 20% of the whole dataset, or only up to 1000\n",
    "        # self.ixes = perm[:num_test] if split == 'test' else perm[num_test:]\n",
    "\n",
    "    def __len__(self):\n",
    "        #return self.ixes.size\n",
    "        return 50_000\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        obs = self.env.reset()\n",
    "        history = [torch.tensor(obs.copy(), dtype=torch.long)]\n",
    "        for _ in range(5):\n",
    "            action = env.action_space.sample()\n",
    "            history.append(torch.tensor([action], dtype=torch.long))\n",
    "            obs, _, _, _ = env.step(action)\n",
    "            history.append(torch.tensor(obs.copy(), dtype=torch.long))\n",
    "        h = torch.cat(([hist for hist in history]))\n",
    "        x = h[:-1]\n",
    "        y = h[1:]\n",
    "        return x, y\n",
    "\n",
    "        obs = torch.tensor(self.env.reset(), dtype=torch.long)\n",
    "        action = self.env.action_space.sample()\n",
    "        next_obs, _, _, _ = self.env.step(action)\n",
    "        x = torch.cat((obs, torch.tensor([action, next_obs[0]], dtype=torch.long)))\n",
    "        y = torch.cat((torch.tensor([-100, -100], dtype=torch.long), torch.tensor(next_obs, dtype=torch.long)))\n",
    "        return x, y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = GymDataset(split='train', env_name='DiscreteHorseshoe-v0')\n",
    "test_dataset = GymDataset(split='test', env_name='DiscreteHorseshoe-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('DiscreteHorseshoe-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([255, 255], dtype=uint8)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 5], dtype=uint8)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([3, 1, 1, 6, 8, 3, 7, 8, 0, 7, 9, 2, 6, 9, 3, 7]),\n",
       " tensor([1, 1, 6, 8, 3, 7, 8, 0, 7, 9, 2, 6, 9, 3, 7, 9]))"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[190] # sample a training instance just to see what one raw example looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/03/2022 18:57:37 - INFO - mingpt.model -   number of parameters: 4.001280e+05\n"
     ]
    }
   ],
   "source": [
    "from mingpt.model import GPT, GPTConfig, GPT1Config\n",
    "\n",
    "# initialize a baby GPT model\n",
    "mconf = GPTConfig(train_dataset.vocab_size, train_dataset.block_size, \n",
    "                  n_layer=2, n_head=4, n_embd=128)\n",
    "model = GPT(mconf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1 iter 97: train loss 0.10494. lr 5.998550e-04: 100%|██████████| 98/98 [00:01<00:00, 86.35it/s]\n",
      "01/03/2022 18:57:41 - INFO - mingpt.trainer -   test loss: 0.041783\n",
      "epoch 2 iter 97: train loss 0.01925. lr 5.994139e-04: 100%|██████████| 98/98 [00:01<00:00, 86.77it/s]\n",
      "01/03/2022 18:57:43 - INFO - mingpt.trainer -   test loss: 0.002651\n",
      "epoch 3 iter 97: train loss 0.03455. lr 5.986774e-04: 100%|██████████| 98/98 [00:01<00:00, 86.44it/s]\n",
      "01/03/2022 18:57:46 - INFO - mingpt.trainer -   test loss: 0.001013\n",
      "epoch 4 iter 97: train loss 0.00785. lr 5.976460e-04: 100%|██████████| 98/98 [00:01<00:00, 81.43it/s]\n",
      "01/03/2022 18:57:48 - INFO - mingpt.trainer -   test loss: 0.000514\n",
      "epoch 5 iter 97: train loss 0.00858. lr 5.963208e-04: 100%|██████████| 98/98 [00:01<00:00, 80.11it/s]\n",
      "01/03/2022 18:57:51 - INFO - mingpt.trainer -   test loss: 0.000469\n",
      "epoch 6 iter 97: train loss 0.03811. lr 5.947032e-04: 100%|██████████| 98/98 [00:01<00:00, 87.74it/s]\n",
      "01/03/2022 18:57:53 - INFO - mingpt.trainer -   test loss: 0.000246\n",
      "epoch 7 iter 97: train loss 0.00488. lr 5.927946e-04: 100%|██████████| 98/98 [00:01<00:00, 80.40it/s]\n",
      "01/03/2022 18:57:56 - INFO - mingpt.trainer -   test loss: 0.000143\n",
      "epoch 8 iter 97: train loss 0.03074. lr 5.905970e-04: 100%|██████████| 98/98 [00:01<00:00, 85.67it/s]\n",
      "01/03/2022 18:57:58 - INFO - mingpt.trainer -   test loss: 0.000123\n",
      "epoch 9 iter 97: train loss 0.00543. lr 5.881126e-04: 100%|██████████| 98/98 [00:01<00:00, 81.97it/s]\n",
      "01/03/2022 18:58:01 - INFO - mingpt.trainer -   test loss: 0.000091\n",
      "epoch 10 iter 97: train loss 0.00163. lr 5.853438e-04: 100%|██████████| 98/98 [00:01<00:00, 84.43it/s]\n",
      "01/03/2022 18:58:03 - INFO - mingpt.trainer -   test loss: 0.000115\n",
      "epoch 11 iter 97: train loss 0.00077. lr 5.822933e-04: 100%|██████████| 98/98 [00:01<00:00, 78.51it/s]\n",
      "01/03/2022 18:58:06 - INFO - mingpt.trainer -   test loss: 0.000245\n",
      "epoch 12 iter 97: train loss 0.00470. lr 5.789642e-04: 100%|██████████| 98/98 [00:01<00:00, 85.14it/s]\n",
      "01/03/2022 18:58:08 - INFO - mingpt.trainer -   test loss: 0.000072\n",
      "epoch 13 iter 97: train loss 0.00149. lr 5.753597e-04: 100%|██████████| 98/98 [00:01<00:00, 81.81it/s]\n",
      "01/03/2022 18:58:11 - INFO - mingpt.trainer -   test loss: 0.000101\n",
      "epoch 14 iter 97: train loss 0.00117. lr 5.714834e-04: 100%|██████████| 98/98 [00:01<00:00, 85.70it/s]\n",
      "01/03/2022 18:58:13 - INFO - mingpt.trainer -   test loss: 0.000041\n",
      "epoch 15 iter 97: train loss 0.00061. lr 5.673392e-04: 100%|██████████| 98/98 [00:01<00:00, 84.19it/s]\n",
      "01/03/2022 18:58:16 - INFO - mingpt.trainer -   test loss: 0.000087\n",
      "epoch 16 iter 97: train loss 0.00116. lr 5.629311e-04: 100%|██████████| 98/98 [00:01<00:00, 86.32it/s]\n",
      "01/03/2022 18:58:18 - INFO - mingpt.trainer -   test loss: 0.000024\n",
      "epoch 17 iter 97: train loss 0.00041. lr 5.582634e-04: 100%|██████████| 98/98 [00:01<00:00, 84.21it/s]\n",
      "01/03/2022 18:58:20 - INFO - mingpt.trainer -   test loss: 0.000037\n",
      "epoch 18 iter 97: train loss 0.00180. lr 5.533408e-04: 100%|██████████| 98/98 [00:01<00:00, 82.77it/s]\n",
      "01/03/2022 18:58:23 - INFO - mingpt.trainer -   test loss: 0.000022\n",
      "epoch 19 iter 97: train loss 0.00017. lr 5.481681e-04: 100%|██████████| 98/98 [00:01<00:00, 83.18it/s]\n",
      "01/03/2022 18:58:25 - INFO - mingpt.trainer -   test loss: 0.000025\n",
      "epoch 20 iter 97: train loss 0.00035. lr 5.427505e-04: 100%|██████████| 98/98 [00:01<00:00, 85.50it/s]\n",
      "01/03/2022 18:58:28 - INFO - mingpt.trainer -   test loss: 0.000010\n"
     ]
    }
   ],
   "source": [
    "from mingpt.trainer import Trainer, TrainerConfig\n",
    "\n",
    "# initialize a trainer instance and kick off training\n",
    "tconf = TrainerConfig(max_epochs=20, batch_size=512, learning_rate=6e-4,\n",
    "                      lr_decay=True, warmup_tokens=1024, final_tokens=50*len(train_dataset)*(4),\n",
    "                      num_workers=4)\n",
    "trainer = Trainer(model, train_dataset, test_dataset, tconf)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 9, 3, 1, 9, 2, 0]], device='cuda:0')"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mingpt.utils import sample\n",
    "\n",
    "sample(model, torch.tensor([0, 9, 3], dtype=torch.long, device=trainer.device)[None, ...], 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0, 10], dtype=uint8)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('DiscreteHorseshoe-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([5, 9], dtype=uint8),\n",
       " -1.0,\n",
       " False,\n",
       " {'state': array([5, 9], dtype=uint8)})"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1, 2, 0], dtype=torch.long, device=trainer.device)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's give the trained model an addition exam\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from mingpt.utils import sample\n",
    "\n",
    "def give_exam(dataset, batch_size=32, max_batches=-1):\n",
    "    \n",
    "    results = []\n",
    "    loader = DataLoader(dataset, batch_size=batch_size)\n",
    "    for b, (x, y) in enumerate(loader):\n",
    "        x = x.to(trainer.device)\n",
    "        d1d2 = x[:, :ndigit*2]\n",
    "        d1d2d3 = sample(model, d1d2, ndigit+1)\n",
    "        d3 = d1d2d3[:, -(ndigit+1):]\n",
    "        factors = torch.tensor([[10**i for i in range(ndigit+1)][::-1]]).to(trainer.device)\n",
    "        # decode the integers from individual digits\n",
    "        d1i = (d1d2[:,:ndigit] * factors[:,1:]).sum(1)\n",
    "        d2i = (d1d2[:,ndigit:ndigit*2] * factors[:,1:]).sum(1)\n",
    "        d3i_pred = (d3 * factors).sum(1)\n",
    "        d3i_gt = d1i + d2i\n",
    "        correct = (d3i_pred == d3i_gt).cpu() # Software 1.0 vs. Software 2.0 fight RIGHT on this line, lol\n",
    "        for i in range(x.size(0)):\n",
    "            results.append(int(correct[i]))\n",
    "            judge = 'YEP!!!' if correct[i] else 'NOPE'\n",
    "            if not correct[i]:\n",
    "                print(\"GPT claims that %03d + %03d = %03d (gt is %03d; %s)\" \n",
    "                      % (d1i[i], d2i[i], d3i_pred[i], d3i_gt[i], judge))\n",
    "        \n",
    "        if max_batches >= 0 and b+1 >= max_batches:\n",
    "            break\n",
    "\n",
    "    print(\"final score: %d/%d = %.2f%% correct\" % (np.sum(results), len(results), 100*np.mean(results)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# training set: how well did we memorize?\n",
    "give_exam(train_dataset, batch_size=1024, max_batches=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test set: how well did we generalize?\n",
    "give_exam(test_dataset, batch_size=1024, max_batches=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# well that's amusing... our model learned everything except 55 + 45"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
